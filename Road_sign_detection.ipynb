{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q gdown\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "file_id = \"1QdLkRuVFErTcmO9ZKNLnzEG1IMZBiDJF\"\n",
        "!gdown --id {file_id} --output dataset.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = \"dataset.zip\"\n",
        "extract_path = \"Signal_Dataset\"  \n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"âœ… Unzipped successfully to:\", extract_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8SwV1jyszSpD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import xml.etree.ElementTree as ET\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set your dataset paths\n",
        "IMG_DIR = \"Signal_Dataset/images\"\n",
        "ANN_DIR = \"Signal_Dataset/annotations\"\n",
        "YOLO_DATASET_DIR = \"YOLO_Format_Dataset\"\n",
        "\n",
        "# Define class labels\n",
        "CLASSES = [\"trafficlight\", \"stop\", \"speedlimit\", \"crosswalk\"]\n",
        "CLASS_TO_ID = {cls: i for i, cls in enumerate(CLASSES)}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "30bj1-T7zSsg"
      },
      "outputs": [],
      "source": [
        "def convert_bbox(size, box):\n",
        "    dw = 1. / size[0]\n",
        "    dh = 1. / size[1]\n",
        "    x_center = (box[0] + box[1]) / 2.0\n",
        "    y_center = (box[2] + box[3]) / 2.0\n",
        "    w = box[1] - box[0]\n",
        "    h = box[3] - box[2]\n",
        "    return x_center * dw, y_center * dh, w * dw, h * dh\n",
        "\n",
        "def convert_annotation(img_path, xml_path):\n",
        "    image = Image.open(img_path)\n",
        "    w, h = image.size\n",
        "    yolo_boxes = []\n",
        "\n",
        "    tree = ET.parse(xml_path)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    for obj in root.iter('object'):\n",
        "        cls = obj.find('name').text.lower()\n",
        "        if cls not in CLASS_TO_ID:\n",
        "            continue\n",
        "        cls_id = CLASS_TO_ID[cls]\n",
        "        xmlbox = obj.find('bndbox')\n",
        "        b = (float(xmlbox.find('xmin').text), float(xmlbox.find('xmax').text),\n",
        "             float(xmlbox.find('ymin').text), float(xmlbox.find('ymax').text))\n",
        "        bbox = convert_bbox((w, h), b)\n",
        "        yolo_boxes.append(f\"{cls_id} {' '.join(map(lambda x: format(x, '.6f'), bbox))}\")\n",
        "    return yolo_boxes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lb8eTh5W0XDM",
        "outputId": "bbc45432-a1c4-4f42-e03f-a06603923c3b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Preparing train set: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 701/701 [00:13<00:00, 50.52it/s]\n",
            "Preparing val set: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 176/176 [00:03<00:00, 52.18it/s]\n"
          ]
        }
      ],
      "source": [
        "def prepare_yolo_dataset(split_ratio=0.2):\n",
        "    all_images = sorted(os.listdir(IMG_DIR))\n",
        "    train_files, val_files = train_test_split(all_images, test_size=split_ratio, random_state=42)\n",
        "\n",
        "    for split, files in zip(['train', 'val'], [train_files, val_files]):\n",
        "        for folder in ['images', 'labels']:\n",
        "            os.makedirs(f\"{YOLO_DATASET_DIR}/{split}/{folder}\", exist_ok=True)\n",
        "\n",
        "        for file in tqdm(files, desc=f\"Preparing {split} set\"):\n",
        "            img_path = os.path.join(IMG_DIR, file)\n",
        "            ann_path = os.path.join(ANN_DIR, file.replace(\".png\", \".xml\"))\n",
        "\n",
        "            yolo_labels = convert_annotation(img_path, ann_path)\n",
        "            with open(f\"{YOLO_DATASET_DIR}/{split}/labels/{file.replace('.png', '.txt')}\", 'w') as f:\n",
        "                f.write(\"\\n\".join(yolo_labels))\n",
        "\n",
        "            shutil.copy(img_path, f\"{YOLO_DATASET_DIR}/{split}/images/{file}\")\n",
        "\n",
        "prepare_yolo_dataset()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OM0qgcXp0cWI"
      },
      "outputs": [],
      "source": [
        "yaml = f\"\"\"\n",
        "path: {YOLO_DATASET_DIR}\n",
        "train: train/images\n",
        "val: val/images\n",
        "\n",
        "names:\n",
        "\"\"\"\n",
        "for i, cls in enumerate(CLASSES):\n",
        "    yaml += f\"  {i}: {cls}\\n\"\n",
        "\n",
        "with open(f\"{YOLO_DATASET_DIR}/data.yaml\", \"w\") as file:\n",
        "    file.write(yaml.strip())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytdwrEFJ0mdT",
        "outputId": "c19e7907-35e2-4754-df19-f51aeac2c8a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m102.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGA4U05q0cSn",
        "outputId": "726fe99a-9889-4819-ba5c-0c562732d259"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8s.pt to 'yolov8s.pt'...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21.5M/21.5M [00:00<00:00, 364MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics 8.3.145 ðŸš€ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8s.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train3, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=10, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/train3, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=4\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
            "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
            "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
            "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
            "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
            " 22        [15, 18, 21]  1   2117596  ultralytics.nn.modules.head.Detect           [4, [128, 256, 512]]          \n",
            "Model summary: 129 layers, 11,137,148 parameters, 11,137,132 gradients, 28.7 GFLOPs\n",
            "\n",
            "Transferred 349/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 2487.4Â±497.3 MB/s, size: 238.4 KB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/YOLO_Format_Dataset/train/labels.cache... 701 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 701/701 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.1 ms, read: 1584.4Â±1558.5 MB/s, size: 311.9 KB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/YOLO_Format_Dataset/val/labels.cache... 176 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 176/176 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Plotting labels to runs/detect/train3/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.00125, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train3\u001b[0m\n",
            "Starting training for 50 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       1/50      12.4G     0.7598       2.09     0.9689         44        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:19<00:00,  2.24it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.19it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        176        260      0.811      0.682      0.729      0.572\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       2/50      3.99G     0.7572     0.8517     0.9639         35        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:17<00:00,  2.55it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.64it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        176        260      0.713      0.647      0.678      0.499\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       3/50      4.53G     0.7686     0.7801     0.9755         34        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:19<00:00,  2.30it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.46it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        176        260      0.653      0.515      0.469      0.351\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       4/50      4.53G     0.7704     0.7532     0.9782         44        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:17<00:00,  2.48it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.67it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        176        260      0.581      0.737      0.636      0.447\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       5/50      4.55G     0.7208     0.6471     0.9707         36        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:15<00:00,  2.77it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:03<00:00,  1.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        176        260      0.814      0.682      0.763      0.579\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       6/50      4.59G     0.7487     0.6688      0.971         40        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:16<00:00,  2.74it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.86it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        176        260      0.678      0.697      0.753       0.58\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       7/50      4.59G     0.7272     0.6072     0.9535         30        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:16<00:00,  2.72it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.66it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        176        260      0.835      0.752      0.828      0.649\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       8/50      4.59G     0.7166      0.602     0.9524         38        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:16<00:00,  2.61it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        176        260      0.859      0.791      0.843      0.671\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       9/50      4.59G     0.7268     0.5734     0.9618         30        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:16<00:00,  2.66it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.68it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        176        260      0.887      0.762      0.857      0.666\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      10/50      4.63G     0.6909     0.5512     0.9495         38        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:16<00:00,  2.74it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.18it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        176        260      0.891      0.762       0.83      0.653\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      11/50      4.66G     0.6725     0.5157     0.9392         27        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:16<00:00,  2.66it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:01<00:00,  3.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        176        260      0.877      0.818      0.887      0.687\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      12/50      4.66G     0.6536     0.5184     0.9298         27        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:16<00:00,  2.70it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.78it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        176        260      0.921      0.852      0.895      0.701\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      13/50      4.66G     0.6542     0.4922     0.9308         38        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:15<00:00,  2.76it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.10it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        176        260      0.807      0.823      0.874      0.679\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      14/50      4.66G     0.6559     0.4888      0.923         29        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:16<00:00,  2.68it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.75it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        176        260      0.901      0.774      0.882      0.706\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      15/50       4.7G     0.6456     0.4657     0.9165         24        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:16<00:00,  2.70it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        176        260      0.854      0.855      0.869      0.688\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      16/50       4.7G     0.6416     0.4745     0.9153         34        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:16<00:00,  2.68it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:03<00:00,  1.75it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        176        260      0.847      0.853      0.884      0.687\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      17/50       4.7G     0.6162     0.4363     0.9178         28        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:16<00:00,  2.69it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.73it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        176        260      0.922      0.874       0.92      0.724\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      18/50       4.7G     0.6184     0.4338      0.904         26        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:15<00:00,  2.78it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.71it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        176        260      0.916      0.896      0.927      0.735\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      19/50       4.7G     0.6129     0.4497     0.9059         32        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:16<00:00,  2.62it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.48it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        176        260      0.892      0.837      0.904      0.715\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      20/50       4.7G     0.6214     0.4434     0.9021         35        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:16<00:00,  2.72it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        176        260      0.897      0.867      0.905      0.725\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      21/50       4.7G     0.6175     0.4368     0.9053         26        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:16<00:00,  2.72it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.25it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        176        260      0.864      0.903      0.926       0.74\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      22/50       4.7G      0.598     0.4247      0.908         30        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:16<00:00,  2.70it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.59it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        176        260      0.921      0.842      0.907      0.726\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      23/50       4.7G     0.5781     0.4043     0.8996         28        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:16<00:00,  2.63it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.83it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        176        260       0.89      0.874      0.901      0.732\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      24/50       4.7G     0.5855      0.398     0.8904         36        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:16<00:00,  2.69it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:03<00:00,  1.76it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        176        260      0.873      0.878      0.904       0.74\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      25/50       4.7G     0.5604     0.3832     0.8929         25        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:16<00:00,  2.72it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.60it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        176        260      0.932      0.899      0.931       0.74\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      26/50       4.7G     0.5853     0.3926     0.8877         33        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:15<00:00,  2.77it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.20it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        176        260      0.867      0.929      0.927      0.762\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      27/50       4.7G     0.5627     0.3848     0.8897         36        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:17<00:00,  2.53it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.46it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        176        260      0.934      0.899      0.925      0.769\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      28/50       4.7G     0.5643     0.3684     0.8854         30        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:16<00:00,  2.68it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        176        260      0.941      0.907      0.937      0.756\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      29/50       4.7G     0.5358     0.3554     0.8803         40        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:16<00:00,  2.64it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:03<00:00,  1.85it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        176        260      0.935      0.925      0.932      0.765\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      30/50       4.7G     0.5313     0.3476     0.8784         38        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:16<00:00,  2.72it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.59it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        176        260      0.913      0.908      0.932      0.775\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      31/50       4.7G     0.5597      0.359     0.8727         20        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:16<00:00,  2.70it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        176        260      0.913      0.922       0.94      0.765\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      32/50       4.7G     0.5348     0.3477      0.883         31        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:16<00:00,  2.60it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        176        260      0.916      0.932      0.941       0.76\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      33/50       4.7G     0.5568     0.3631     0.8887         33        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:16<00:00,  2.72it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        176        260      0.923       0.93      0.947      0.778\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      34/50       4.7G     0.5371     0.3347     0.8698         35        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:16<00:00,  2.68it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.19it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        176        260      0.939      0.893      0.938      0.773\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      35/50       4.7G     0.5256     0.3248     0.8682         35        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:16<00:00,  2.65it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.94it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        176        260       0.93      0.904      0.938      0.778\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      36/50       4.7G     0.5342     0.3353     0.8761         31        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:16<00:00,  2.72it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.67it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        176        260      0.944      0.925      0.939      0.771\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      37/50       4.7G     0.5169      0.321     0.8666         28        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:16<00:00,  2.73it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:03<00:00,  1.93it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        176        260      0.954      0.928      0.949      0.787\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      38/50       4.7G      0.504     0.3164     0.8645         32        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:16<00:00,  2.63it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.65it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        176        260       0.93      0.937      0.946      0.783\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      39/50       4.7G     0.5049     0.3192     0.8659         51        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:15<00:00,  2.77it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.55it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        176        260      0.954      0.924      0.946      0.792\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      40/50       4.7G     0.4851     0.3044     0.8616         35        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:17<00:00,  2.53it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.62it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        176        260      0.918      0.928      0.933      0.781\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      41/50       4.7G     0.4805     0.2791     0.8405         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:17<00:00,  2.53it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.56it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        176        260      0.945      0.916      0.938      0.782\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      42/50       4.7G     0.4887     0.2815     0.8359         20        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:17<00:00,  2.49it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.76it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        176        260      0.942      0.909      0.935      0.788\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      43/50       4.7G     0.4759     0.2756     0.8274         19        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:15<00:00,  2.80it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        176        260      0.931      0.924      0.935      0.792\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      44/50       4.7G     0.4703     0.2648     0.8378         14        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:16<00:00,  2.66it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:03<00:00,  1.94it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        176        260      0.922      0.941      0.937      0.785\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      45/50       4.7G     0.4677     0.2666      0.839         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:15<00:00,  2.84it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.72it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        176        260      0.919      0.935      0.938      0.787\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      46/50       4.7G     0.4629     0.2617     0.8298         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:16<00:00,  2.70it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:01<00:00,  3.05it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        176        260      0.941      0.913      0.934        0.8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      47/50       4.7G      0.451     0.2547      0.829         19        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:16<00:00,  2.67it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.07it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        176        260      0.954      0.922      0.946      0.798\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      48/50       4.7G     0.4493     0.2469      0.815         15        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:15<00:00,  2.82it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.91it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        176        260      0.931      0.918      0.943      0.798\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      49/50       4.7G     0.4409     0.2446     0.8325         15        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:16<00:00,  2.75it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.78it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        176        260      0.934      0.928      0.944      0.804\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      50/50       4.7G     0.4321     0.2403     0.8231         18        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:15<00:00,  2.76it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        176        260      0.948      0.922      0.943      0.803\n",
            "\n",
            "50 epochs completed in 0.276 hours.\n",
            "Optimizer stripped from runs/detect/train3/weights/last.pt, 22.5MB\n",
            "Optimizer stripped from runs/detect/train3/weights/best.pt, 22.5MB\n",
            "\n",
            "Validating runs/detect/train3/weights/best.pt...\n",
            "Ultralytics 8.3.145 ðŸš€ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 72 layers, 11,127,132 parameters, 0 gradients, 28.4 GFLOPs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:03<00:00,  1.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        176        260      0.934      0.928      0.944      0.804\n",
            "          trafficlight         23         44      0.922      0.818      0.856      0.584\n",
            "                  stop         28         28      0.896      0.964      0.982      0.904\n",
            "            speedlimit        124        141      0.984          1      0.988      0.926\n",
            "             crosswalk         41         47      0.936      0.931      0.948      0.802\n",
            "Speed: 0.3ms preprocess, 4.1ms inference, 0.0ms loss, 4.9ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/train3\u001b[0m\n",
            "Ultralytics 8.3.145 ðŸš€ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 72 layers, 11,127,132 parameters, 0 gradients, 28.4 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 2710.7Â±638.9 MB/s, size: 231.5 KB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/YOLO_Format_Dataset/val/labels.cache... 176 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 176/176 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:04<00:00,  2.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        176        260      0.943      0.917      0.943      0.805\n",
            "          trafficlight         23         44       0.93      0.795      0.857      0.579\n",
            "                  stop         28         28      0.923      0.964       0.98      0.905\n",
            "            speedlimit        124        141      0.986      0.998      0.988      0.927\n",
            "             crosswalk         41         47      0.935      0.912      0.948      0.808\n",
            "Speed: 2.1ms preprocess, 7.8ms inference, 0.0ms loss, 2.3ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/train32\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO(\"yolov8s.pt\")\n",
        "model.train(data=f\"{YOLO_DATASET_DIR}data.yaml\", epochs=50, imgsz=640, batch=16,patience=10)\n",
        "\n",
        "# Evaluate\n",
        "metrics = model.val()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wdWY-eLJY09"
      },
      "outputs": [],
      "source": [
        "print(\"\\nðŸ“Š Evaluation Metrics:\")\n",
        "print(f\"mAP@0.5: {metrics.box.map50:.3f}\")\n",
        "print(f\"mAP@0.5:0.95: {metrics.box.map:.3f}\")\n",
        "print(f\"Precision: {metrics.box.precision:.3f}\")\n",
        "print(f\"Recall: {metrics.box.recall:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road102.png: 448x640 1 speedlimit, 168.4ms\n",
            "Speed: 2.4ms preprocess, 168.4ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road107.png: 640x512 1 speedlimit, 204.1ms\n",
            "Speed: 2.0ms preprocess, 204.1ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road119.png: 448x640 1 speedlimit, 167.1ms\n",
            "Speed: 2.2ms preprocess, 167.1ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road120.png: 480x640 1 speedlimit, 193.8ms\n",
            "Speed: 1.8ms preprocess, 193.8ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road124.png: 640x544 1 trafficlight, 1 crosswalk, 262.2ms\n",
            "Speed: 2.7ms preprocess, 262.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 544)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road125.png: 640x448 1 crosswalk, 193.1ms\n",
            "Speed: 2.0ms preprocess, 193.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road126.png: 640x448 1 crosswalk, 194.2ms\n",
            "Speed: 1.7ms preprocess, 194.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road128.png: 448x640 1 crosswalk, 243.4ms\n",
            "Speed: 3.3ms preprocess, 243.4ms inference, 1.9ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road133.png: 384x640 1 crosswalk, 195.6ms\n",
            "Speed: 1.9ms preprocess, 195.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road138.png: 480x640 1 crosswalk, 187.1ms\n",
            "Speed: 2.9ms preprocess, 187.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road142.png: 640x448 1 crosswalk, 204.5ms\n",
            "Speed: 2.1ms preprocess, 204.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road147.png: 384x640 1 crosswalk, 151.2ms\n",
            "Speed: 1.9ms preprocess, 151.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road155.png: 640x640 1 crosswalk, 264.7ms\n",
            "Speed: 4.6ms preprocess, 264.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road157.png: 640x480 1 speedlimit, 202.8ms\n",
            "Speed: 2.6ms preprocess, 202.8ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road158.png: 640x480 1 crosswalk, 196.8ms\n",
            "Speed: 2.7ms preprocess, 196.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road159.png: 640x480 1 speedlimit, 182.8ms\n",
            "Speed: 2.5ms preprocess, 182.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road161.png: 640x480 1 speedlimit, 180.6ms\n",
            "Speed: 2.1ms preprocess, 180.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road163.png: 640x480 2 stops, 2 speedlimits, 198.7ms\n",
            "Speed: 2.8ms preprocess, 198.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road167.png: 640x480 1 stop, 3 crosswalks, 188.9ms\n",
            "Speed: 2.3ms preprocess, 188.9ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road168.png: 640x480 1 crosswalk, 238.7ms\n",
            "Speed: 2.2ms preprocess, 238.7ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road169.png: 640x480 1 speedlimit, 202.8ms\n",
            "Speed: 2.4ms preprocess, 202.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road171.png: 640x480 1 crosswalk, 209.6ms\n",
            "Speed: 2.2ms preprocess, 209.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road172.png: 640x480 1 speedlimit, 193.2ms\n",
            "Speed: 2.8ms preprocess, 193.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road174.png: 640x480 1 speedlimit, 191.9ms\n",
            "Speed: 2.5ms preprocess, 191.9ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road176.png: 640x480 1 trafficlight, 1 crosswalk, 175.5ms\n",
            "Speed: 2.2ms preprocess, 175.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road185.png: 640x480 (no detections), 172.0ms\n",
            "Speed: 2.3ms preprocess, 172.0ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road186.png: 640x480 1 crosswalk, 224.5ms\n",
            "Speed: 2.6ms preprocess, 224.5ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road197.png: 640x480 1 speedlimit, 203.9ms\n",
            "Speed: 2.2ms preprocess, 203.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road198.png: 640x480 1 speedlimit, 228.5ms\n",
            "Speed: 2.9ms preprocess, 228.5ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road204.png: 640x480 1 crosswalk, 196.4ms\n",
            "Speed: 2.5ms preprocess, 196.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road206.png: 640x480 1 speedlimit, 232.4ms\n",
            "Speed: 2.7ms preprocess, 232.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road220.png: 640x480 1 speedlimit, 251.5ms\n",
            "Speed: 2.6ms preprocess, 251.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road221.png: 640x480 1 speedlimit, 201.7ms\n",
            "Speed: 2.4ms preprocess, 201.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road223.png: 640x480 1 speedlimit, 189.6ms\n",
            "Speed: 2.3ms preprocess, 189.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road25.png: 448x640 4 trafficlights, 171.5ms\n",
            "Speed: 2.2ms preprocess, 171.5ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road255.png: 640x480 1 speedlimit, 185.6ms\n",
            "Speed: 2.2ms preprocess, 185.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road271.png: 640x480 1 speedlimit, 180.9ms\n",
            "Speed: 2.2ms preprocess, 180.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road275.png: 640x480 1 trafficlight, 1 stop, 1 crosswalk, 182.2ms\n",
            "Speed: 2.2ms preprocess, 182.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road277.png: 640x480 1 speedlimit, 207.3ms\n",
            "Speed: 2.1ms preprocess, 207.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road286.png: 640x480 1 speedlimit, 180.0ms\n",
            "Speed: 2.2ms preprocess, 180.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road288.png: 640x480 1 speedlimit, 190.7ms\n",
            "Speed: 2.4ms preprocess, 190.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road289.png: 640x480 1 trafficlight, 1 stop, 189.1ms\n",
            "Speed: 2.3ms preprocess, 189.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road290.png: 640x480 1 speedlimit, 179.0ms\n",
            "Speed: 2.1ms preprocess, 179.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road292.png: 640x480 1 trafficlight, 2 speedlimits, 171.7ms\n",
            "Speed: 2.1ms preprocess, 171.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road295.png: 640x480 1 speedlimit, 184.8ms\n",
            "Speed: 2.1ms preprocess, 184.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road302.png: 640x480 1 stop, 1 speedlimit, 176.5ms\n",
            "Speed: 2.1ms preprocess, 176.5ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road306.png: 640x480 1 stop, 182.0ms\n",
            "Speed: 2.1ms preprocess, 182.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road31.png: 480x640 2 trafficlights, 1 stop, 174.0ms\n",
            "Speed: 3.9ms preprocess, 174.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road313.png: 640x480 1 speedlimit, 181.9ms\n",
            "Speed: 2.3ms preprocess, 181.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road318.png: 640x480 1 speedlimit, 1 crosswalk, 189.7ms\n",
            "Speed: 2.2ms preprocess, 189.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road320.png: 640x480 1 speedlimit, 180.5ms\n",
            "Speed: 2.0ms preprocess, 180.5ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road327.png: 640x480 1 trafficlight, 1 speedlimit, 1 crosswalk, 177.6ms\n",
            "Speed: 2.1ms preprocess, 177.6ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road332.png: 640x480 1 speedlimit, 188.7ms\n",
            "Speed: 2.2ms preprocess, 188.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road337.png: 640x480 1 speedlimit, 178.6ms\n",
            "Speed: 2.1ms preprocess, 178.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road338.png: 640x480 1 speedlimit, 185.2ms\n",
            "Speed: 2.2ms preprocess, 185.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road346.png: 640x480 1 speedlimit, 179.8ms\n",
            "Speed: 2.2ms preprocess, 179.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road350.png: 640x480 1 speedlimit, 179.1ms\n",
            "Speed: 2.2ms preprocess, 179.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road351.png: 640x480 1 speedlimit, 183.6ms\n",
            "Speed: 2.3ms preprocess, 183.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road356.png: 640x480 1 stop, 181.4ms\n",
            "Speed: 2.2ms preprocess, 181.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road360.png: 640x480 1 speedlimit, 189.9ms\n",
            "Speed: 2.4ms preprocess, 189.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road361.png: 640x480 1 speedlimit, 1 crosswalk, 188.5ms\n",
            "Speed: 2.2ms preprocess, 188.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road367.png: 640x480 1 speedlimit, 1 crosswalk, 187.3ms\n",
            "Speed: 2.2ms preprocess, 187.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road368.png: 640x480 1 speedlimit, 191.6ms\n",
            "Speed: 2.2ms preprocess, 191.6ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road373.png: 640x480 2 speedlimits, 208.4ms\n",
            "Speed: 2.4ms preprocess, 208.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road374.png: 640x480 1 stop, 229.2ms\n",
            "Speed: 2.3ms preprocess, 229.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road377.png: 640x480 1 speedlimit, 241.4ms\n",
            "Speed: 3.4ms preprocess, 241.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road379.png: 640x480 1 speedlimit, 231.2ms\n",
            "Speed: 2.2ms preprocess, 231.2ms inference, 3.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road38.png: 448x640 1 trafficlight, 273.7ms\n",
            "Speed: 3.7ms preprocess, 273.7ms inference, 2.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road383.png: 640x480 1 speedlimit, 230.9ms\n",
            "Speed: 2.5ms preprocess, 230.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road386.png: 640x480 1 speedlimit, 260.0ms\n",
            "Speed: 2.3ms preprocess, 260.0ms inference, 3.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road39.png: 640x512 1 trafficlight, 259.5ms\n",
            "Speed: 3.7ms preprocess, 259.5ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road392.png: 640x480 1 speedlimit, 278.6ms\n",
            "Speed: 2.2ms preprocess, 278.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road393.png: 640x480 1 speedlimit, 241.1ms\n",
            "Speed: 2.4ms preprocess, 241.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road394.png: 640x480 1 speedlimit, 249.9ms\n",
            "Speed: 2.9ms preprocess, 249.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road406.png: 640x480 1 speedlimit, 323.2ms\n",
            "Speed: 4.7ms preprocess, 323.2ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road41.png: 640x448 2 trafficlights, 315.3ms\n",
            "Speed: 4.4ms preprocess, 315.3ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road413.png: 640x480 1 speedlimit, 271.0ms\n",
            "Speed: 2.5ms preprocess, 271.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road415.png: 640x480 1 speedlimit, 222.5ms\n",
            "Speed: 2.2ms preprocess, 222.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road418.png: 640x480 1 speedlimit, 236.9ms\n",
            "Speed: 2.4ms preprocess, 236.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road422.png: 640x480 1 speedlimit, 238.9ms\n",
            "Speed: 2.3ms preprocess, 238.9ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road423.png: 640x480 1 speedlimit, 215.7ms\n",
            "Speed: 2.1ms preprocess, 215.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road427.png: 640x480 1 speedlimit, 265.8ms\n",
            "Speed: 4.2ms preprocess, 265.8ms inference, 4.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road43.png: 448x640 5 trafficlights, 210.3ms\n",
            "Speed: 2.0ms preprocess, 210.3ms inference, 1.9ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road440.png: 640x480 1 speedlimit, 247.0ms\n",
            "Speed: 2.4ms preprocess, 247.0ms inference, 4.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road441.png: 640x480 1 speedlimit, 298.4ms\n",
            "Speed: 3.9ms preprocess, 298.4ms inference, 2.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road455.png: 640x480 1 speedlimit, 269.8ms\n",
            "Speed: 3.9ms preprocess, 269.8ms inference, 3.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road457.png: 640x480 1 speedlimit, 273.9ms\n",
            "Speed: 2.8ms preprocess, 273.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road463.png: 640x480 1 trafficlight, 1 speedlimit, 1 crosswalk, 242.9ms\n",
            "Speed: 2.6ms preprocess, 242.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road466.png: 640x480 1 trafficlight, 1 speedlimit, 1 crosswalk, 202.7ms\n",
            "Speed: 2.1ms preprocess, 202.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road467.png: 640x480 1 speedlimit, 223.9ms\n",
            "Speed: 2.9ms preprocess, 223.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road473.png: 640x480 1 speedlimit, 255.9ms\n",
            "Speed: 4.4ms preprocess, 255.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road479.png: 640x480 1 speedlimit, 229.2ms\n",
            "Speed: 2.1ms preprocess, 229.2ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road481.png: 640x480 1 speedlimit, 1 crosswalk, 199.8ms\n",
            "Speed: 2.4ms preprocess, 199.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road484.png: 640x480 1 speedlimit, 216.5ms\n",
            "Speed: 3.4ms preprocess, 216.5ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road486.png: 640x480 1 speedlimit, 251.4ms\n",
            "Speed: 2.8ms preprocess, 251.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road487.png: 640x480 1 speedlimit, 250.4ms\n",
            "Speed: 2.1ms preprocess, 250.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road495.png: 640x480 1 trafficlight, 1 crosswalk, 207.5ms\n",
            "Speed: 2.7ms preprocess, 207.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road5.png: 640x448 1 trafficlight, 200.0ms\n",
            "Speed: 2.5ms preprocess, 200.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road500.png: 640x480 1 speedlimit, 189.0ms\n",
            "Speed: 2.6ms preprocess, 189.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road501.png: 640x480 1 speedlimit, 180.9ms\n",
            "Speed: 2.5ms preprocess, 180.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road51.png: 640x640 1 trafficlight, 243.2ms\n",
            "Speed: 6.0ms preprocess, 243.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road524.png: 640x480 1 speedlimit, 182.1ms\n",
            "Speed: 2.2ms preprocess, 182.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road536.png: 640x480 1 speedlimit, 1 crosswalk, 183.1ms\n",
            "Speed: 2.1ms preprocess, 183.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road54.png: 640x480 1 stop, 178.4ms\n",
            "Speed: 2.0ms preprocess, 178.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road549.png: 640x480 1 speedlimit, 1 crosswalk, 167.0ms\n",
            "Speed: 2.1ms preprocess, 167.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road55.png: 448x640 1 stop, 152.4ms\n",
            "Speed: 1.9ms preprocess, 152.4ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road555.png: 640x480 1 speedlimit, 207.5ms\n",
            "Speed: 2.0ms preprocess, 207.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road566.png: 640x480 1 speedlimit, 172.3ms\n",
            "Speed: 2.1ms preprocess, 172.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road569.png: 640x480 1 speedlimit, 177.8ms\n",
            "Speed: 2.7ms preprocess, 177.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road57.png: 640x480 1 stop, 208.8ms\n",
            "Speed: 2.2ms preprocess, 208.8ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road574.png: 640x480 1 speedlimit, 1 crosswalk, 226.8ms\n",
            "Speed: 3.3ms preprocess, 226.8ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road575.png: 640x480 1 speedlimit, 1 crosswalk, 204.5ms\n",
            "Speed: 2.2ms preprocess, 204.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road580.png: 640x480 2 crosswalks, 202.2ms\n",
            "Speed: 2.2ms preprocess, 202.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road584.png: 640x480 1 speedlimit, 184.7ms\n",
            "Speed: 2.3ms preprocess, 184.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road595.png: 640x480 1 speedlimit, 204.2ms\n",
            "Speed: 2.6ms preprocess, 204.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road60.png: 640x640 2 stops, 262.5ms\n",
            "Speed: 5.6ms preprocess, 262.5ms inference, 9.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road61.png: 512x640 1 stop, 241.7ms\n",
            "Speed: 2.2ms preprocess, 241.7ms inference, 1.8ms postprocess per image at shape (1, 3, 512, 640)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road610.png: 640x480 1 speedlimit, 199.6ms\n",
            "Speed: 2.7ms preprocess, 199.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road616.png: 640x480 2 speedlimits, 1 crosswalk, 184.2ms\n",
            "Speed: 2.0ms preprocess, 184.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road617.png: 640x480 1 speedlimit, 1 crosswalk, 169.6ms\n",
            "Speed: 2.4ms preprocess, 169.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road629.png: 640x480 1 speedlimit, 175.1ms\n",
            "Speed: 2.0ms preprocess, 175.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road632.png: 640x480 1 stop, 181.3ms\n",
            "Speed: 2.2ms preprocess, 181.3ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road64.png: 512x640 1 stop, 179.5ms\n",
            "Speed: 3.2ms preprocess, 179.5ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road652.png: 640x480 2 speedlimits, 1 crosswalk, 169.9ms\n",
            "Speed: 2.2ms preprocess, 169.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road653.png: 640x480 2 speedlimits, 1 crosswalk, 180.2ms\n",
            "Speed: 2.1ms preprocess, 180.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road658.png: 640x480 2 speedlimits, 1 crosswalk, 170.8ms\n",
            "Speed: 2.2ms preprocess, 170.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road661.png: 640x480 1 speedlimit, 164.4ms\n",
            "Speed: 2.4ms preprocess, 164.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road665.png: 640x480 1 speedlimit, 179.1ms\n",
            "Speed: 2.3ms preprocess, 179.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road669.png: 640x480 1 speedlimit, 1 crosswalk, 169.2ms\n",
            "Speed: 2.2ms preprocess, 169.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road673.png: 640x480 1 stop, 1 speedlimit, 170.4ms\n",
            "Speed: 2.2ms preprocess, 170.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road675.png: 640x480 1 stop, 1 speedlimit, 169.0ms\n",
            "Speed: 2.5ms preprocess, 169.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road678.png: 640x480 1 speedlimit, 171.5ms\n",
            "Speed: 2.2ms preprocess, 171.5ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road683.png: 640x480 1 speedlimit, 169.5ms\n",
            "Speed: 2.1ms preprocess, 169.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road684.png: 640x480 1 speedlimit, 169.1ms\n",
            "Speed: 2.2ms preprocess, 169.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road687.png: 640x480 2 speedlimits, 168.0ms\n",
            "Speed: 2.3ms preprocess, 168.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road692.png: 640x480 2 speedlimits, 174.3ms\n",
            "Speed: 2.5ms preprocess, 174.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road7.png: 640x512 2 trafficlights, 185.1ms\n",
            "Speed: 2.3ms preprocess, 185.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road703.png: 640x480 2 speedlimits, 172.5ms\n",
            "Speed: 2.1ms preprocess, 172.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road706.png: 640x480 2 speedlimits, 203.1ms\n",
            "Speed: 3.4ms preprocess, 203.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road708.png: 640x480 2 speedlimits, 170.2ms\n",
            "Speed: 4.4ms preprocess, 170.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road71.png: 512x640 1 stop, 185.2ms\n",
            "Speed: 2.8ms preprocess, 185.2ms inference, 1.9ms postprocess per image at shape (1, 3, 512, 640)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road712.png: 640x480 2 speedlimits, 178.5ms\n",
            "Speed: 2.2ms preprocess, 178.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road735.png: 640x480 3 speedlimits, 173.0ms\n",
            "Speed: 2.4ms preprocess, 173.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road738.png: 640x480 2 speedlimits, 172.1ms\n",
            "Speed: 2.3ms preprocess, 172.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road742.png: 640x480 2 speedlimits, 175.6ms\n",
            "Speed: 2.2ms preprocess, 175.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road743.png: 640x480 2 speedlimits, 150.3ms\n",
            "Speed: 3.3ms preprocess, 150.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road749.png: 640x480 2 speedlimits, 199.0ms\n",
            "Speed: 2.1ms preprocess, 199.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road754.png: 640x480 2 speedlimits, 172.0ms\n",
            "Speed: 2.1ms preprocess, 172.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road757.png: 640x480 1 speedlimit, 175.1ms\n",
            "Speed: 2.1ms preprocess, 175.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road76.png: 448x640 1 stop, 215.9ms\n",
            "Speed: 2.2ms preprocess, 215.9ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road760.png: 640x480 1 speedlimit, 184.4ms\n",
            "Speed: 2.5ms preprocess, 184.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road763.png: 640x480 1 speedlimit, 198.3ms\n",
            "Speed: 2.3ms preprocess, 198.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road766.png: 640x480 1 speedlimit, 234.8ms\n",
            "Speed: 2.5ms preprocess, 234.8ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road779.png: 640x480 1 speedlimit, 264.0ms\n",
            "Speed: 2.1ms preprocess, 264.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road783.png: 640x480 1 speedlimit, 220.5ms\n",
            "Speed: 2.2ms preprocess, 220.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road795.png: 640x480 1 speedlimit, 259.5ms\n",
            "Speed: 2.8ms preprocess, 259.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road801.png: 640x480 1 speedlimit, 203.8ms\n",
            "Speed: 2.2ms preprocess, 203.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road808.png: 640x480 1 speedlimit, 248.8ms\n",
            "Speed: 2.3ms preprocess, 248.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road81.png: 640x480 1 stop, 248.2ms\n",
            "Speed: 2.0ms preprocess, 248.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road820.png: 640x480 7 trafficlights, 1 stop, 2 crosswalks, 266.7ms\n",
            "Speed: 2.5ms preprocess, 266.7ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road825.png: 640x480 3 trafficlights, 1 stop, 2 crosswalks, 258.7ms\n",
            "Speed: 3.3ms preprocess, 258.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road826.png: 640x480 2 trafficlights, 1 stop, 2 crosswalks, 263.5ms\n",
            "Speed: 2.4ms preprocess, 263.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road827.png: 640x480 1 speedlimit, 285.0ms\n",
            "Speed: 6.2ms preprocess, 285.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road828.png: 640x480 1 speedlimit, 223.0ms\n",
            "Speed: 2.8ms preprocess, 223.0ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road838.png: 640x480 1 speedlimit, 1 crosswalk, 259.4ms\n",
            "Speed: 5.5ms preprocess, 259.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road84.png: 640x448 1 stop, 259.2ms\n",
            "Speed: 2.4ms preprocess, 259.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road841.png: 640x480 1 speedlimit, 1 crosswalk, 252.3ms\n",
            "Speed: 2.6ms preprocess, 252.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road845.png: 640x480 1 trafficlight, 1 speedlimit, 1 crosswalk, 243.2ms\n",
            "Speed: 7.0ms preprocess, 243.2ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road851.png: 640x480 1 speedlimit, 250.3ms\n",
            "Speed: 2.3ms preprocess, 250.3ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road854.png: 640x480 1 speedlimit, 242.5ms\n",
            "Speed: 3.5ms preprocess, 242.5ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road867.png: 640x480 1 speedlimit, 216.2ms\n",
            "Speed: 2.8ms preprocess, 216.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road875.png: 640x480 1 stop, 2 speedlimits, 217.1ms\n",
            "Speed: 2.1ms preprocess, 217.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road876.png: 640x480 1 stop, 1 speedlimit, 226.3ms\n",
            "Speed: 3.1ms preprocess, 226.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road91.png: 640x640 1 stop, 255.3ms\n",
            "Speed: 6.0ms preprocess, 255.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road94.png: 512x640 1 stop, 250.1ms\n",
            "Speed: 2.3ms preprocess, 250.1ms inference, 1.6ms postprocess per image at shape (1, 3, 512, 640)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road95.png: 640x480 1 stop, 419.9ms\n",
            "Speed: 2.9ms preprocess, 419.9ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road102.png: 448x640 1 speedlimit, 193.8ms\n",
            "Speed: 2.1ms preprocess, 193.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road107.png: 640x512 1 speedlimit, 210.8ms\n",
            "Speed: 2.7ms preprocess, 210.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road119.png: 448x640 1 speedlimit, 191.2ms\n",
            "Speed: 2.0ms preprocess, 191.2ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road120.png: 480x640 1 speedlimit, 306.6ms\n",
            "Speed: 2.0ms preprocess, 306.6ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road124.png: 640x544 1 trafficlight, 1 crosswalk, 246.7ms\n",
            "Speed: 3.8ms preprocess, 246.7ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 544)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road125.png: 640x448 1 crosswalk, 209.8ms\n",
            "Speed: 2.0ms preprocess, 209.8ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road126.png: 640x448 1 crosswalk, 198.8ms\n",
            "Speed: 2.0ms preprocess, 198.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road128.png: 448x640 1 crosswalk, 195.1ms\n",
            "Speed: 2.7ms preprocess, 195.1ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road133.png: 384x640 1 crosswalk, 182.4ms\n",
            "Speed: 1.7ms preprocess, 182.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road138.png: 480x640 1 crosswalk, 204.9ms\n",
            "Speed: 2.9ms preprocess, 204.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road142.png: 640x448 1 crosswalk, 188.6ms\n",
            "Speed: 2.4ms preprocess, 188.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road147.png: 384x640 1 crosswalk, 172.3ms\n",
            "Speed: 2.0ms preprocess, 172.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road155.png: 640x640 1 crosswalk, 285.4ms\n",
            "Speed: 4.1ms preprocess, 285.4ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road157.png: 640x480 1 speedlimit, 226.3ms\n",
            "Speed: 2.4ms preprocess, 226.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road158.png: 640x480 1 crosswalk, 222.7ms\n",
            "Speed: 2.3ms preprocess, 222.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road159.png: 640x480 1 speedlimit, 221.8ms\n",
            "Speed: 2.3ms preprocess, 221.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road161.png: 640x480 1 speedlimit, 205.9ms\n",
            "Speed: 3.2ms preprocess, 205.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road163.png: 640x480 2 stops, 2 speedlimits, 187.4ms\n",
            "Speed: 2.2ms preprocess, 187.4ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road167.png: 640x480 1 stop, 3 crosswalks, 292.9ms\n",
            "Speed: 3.8ms preprocess, 292.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road168.png: 640x480 1 crosswalk, 208.7ms\n",
            "Speed: 3.0ms preprocess, 208.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road169.png: 640x480 1 speedlimit, 205.6ms\n",
            "Speed: 4.4ms preprocess, 205.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road171.png: 640x480 1 crosswalk, 184.1ms\n",
            "Speed: 2.8ms preprocess, 184.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road172.png: 640x480 1 speedlimit, 175.3ms\n",
            "Speed: 2.1ms preprocess, 175.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road174.png: 640x480 1 speedlimit, 191.6ms\n",
            "Speed: 2.7ms preprocess, 191.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road176.png: 640x480 1 trafficlight, 1 crosswalk, 173.4ms\n",
            "Speed: 2.4ms preprocess, 173.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road185.png: 640x480 (no detections), 319.8ms\n",
            "Speed: 4.3ms preprocess, 319.8ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road186.png: 640x480 1 crosswalk, 233.6ms\n",
            "Speed: 2.7ms preprocess, 233.6ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road197.png: 640x480 1 speedlimit, 172.8ms\n",
            "Speed: 2.2ms preprocess, 172.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road198.png: 640x480 1 speedlimit, 169.8ms\n",
            "Speed: 2.1ms preprocess, 169.8ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road204.png: 640x480 1 crosswalk, 176.9ms\n",
            "Speed: 2.1ms preprocess, 176.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road206.png: 640x480 1 speedlimit, 174.2ms\n",
            "Speed: 2.2ms preprocess, 174.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road220.png: 640x480 1 speedlimit, 172.9ms\n",
            "Speed: 2.1ms preprocess, 172.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road221.png: 640x480 1 speedlimit, 167.5ms\n",
            "Speed: 2.2ms preprocess, 167.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road223.png: 640x480 1 speedlimit, 174.5ms\n",
            "Speed: 2.1ms preprocess, 174.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road25.png: 448x640 4 trafficlights, 155.1ms\n",
            "Speed: 2.3ms preprocess, 155.1ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road255.png: 640x480 1 speedlimit, 169.5ms\n",
            "Speed: 2.2ms preprocess, 169.5ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road271.png: 640x480 1 speedlimit, 170.8ms\n",
            "Speed: 2.3ms preprocess, 170.8ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road275.png: 640x480 1 trafficlight, 1 stop, 1 crosswalk, 171.8ms\n",
            "Speed: 2.2ms preprocess, 171.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road277.png: 640x480 1 speedlimit, 166.4ms\n",
            "Speed: 2.2ms preprocess, 166.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road286.png: 640x480 1 speedlimit, 170.6ms\n",
            "Speed: 2.1ms preprocess, 170.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road288.png: 640x480 1 speedlimit, 169.4ms\n",
            "Speed: 2.1ms preprocess, 169.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road289.png: 640x480 1 trafficlight, 1 stop, 170.8ms\n",
            "Speed: 2.2ms preprocess, 170.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road290.png: 640x480 1 speedlimit, 191.9ms\n",
            "Speed: 2.2ms preprocess, 191.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road292.png: 640x480 1 trafficlight, 2 speedlimits, 166.1ms\n",
            "Speed: 2.2ms preprocess, 166.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road295.png: 640x480 1 speedlimit, 171.8ms\n",
            "Speed: 2.1ms preprocess, 171.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road302.png: 640x480 1 stop, 1 speedlimit, 164.4ms\n",
            "Speed: 2.1ms preprocess, 164.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road306.png: 640x480 1 stop, 166.2ms\n",
            "Speed: 2.1ms preprocess, 166.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road31.png: 480x640 2 trafficlights, 1 stop, 187.4ms\n",
            "Speed: 2.2ms preprocess, 187.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road313.png: 640x480 1 speedlimit, 171.9ms\n",
            "Speed: 2.2ms preprocess, 171.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road318.png: 640x480 1 speedlimit, 1 crosswalk, 172.4ms\n",
            "Speed: 2.2ms preprocess, 172.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road320.png: 640x480 1 speedlimit, 176.3ms\n",
            "Speed: 2.1ms preprocess, 176.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road327.png: 640x480 1 trafficlight, 1 speedlimit, 1 crosswalk, 181.9ms\n",
            "Speed: 2.1ms preprocess, 181.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road332.png: 640x480 1 speedlimit, 190.4ms\n",
            "Speed: 2.3ms preprocess, 190.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road337.png: 640x480 1 speedlimit, 265.6ms\n",
            "Speed: 2.3ms preprocess, 265.6ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road338.png: 640x480 1 speedlimit, 209.3ms\n",
            "Speed: 2.2ms preprocess, 209.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road346.png: 640x480 1 speedlimit, 189.9ms\n",
            "Speed: 2.2ms preprocess, 189.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road350.png: 640x480 1 speedlimit, 188.5ms\n",
            "Speed: 2.3ms preprocess, 188.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road351.png: 640x480 1 speedlimit, 171.6ms\n",
            "Speed: 2.2ms preprocess, 171.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road356.png: 640x480 1 stop, 175.0ms\n",
            "Speed: 2.1ms preprocess, 175.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road360.png: 640x480 1 speedlimit, 166.8ms\n",
            "Speed: 2.2ms preprocess, 166.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road361.png: 640x480 1 speedlimit, 1 crosswalk, 198.0ms\n",
            "Speed: 2.4ms preprocess, 198.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road367.png: 640x480 1 speedlimit, 1 crosswalk, 203.2ms\n",
            "Speed: 2.7ms preprocess, 203.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road368.png: 640x480 1 speedlimit, 242.4ms\n",
            "Speed: 2.2ms preprocess, 242.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road373.png: 640x480 2 speedlimits, 171.9ms\n",
            "Speed: 2.3ms preprocess, 171.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road374.png: 640x480 1 stop, 177.6ms\n",
            "Speed: 2.2ms preprocess, 177.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road377.png: 640x480 1 speedlimit, 226.8ms\n",
            "Speed: 2.7ms preprocess, 226.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road379.png: 640x480 1 speedlimit, 179.7ms\n",
            "Speed: 2.1ms preprocess, 179.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road38.png: 448x640 1 trafficlight, 199.8ms\n",
            "Speed: 5.5ms preprocess, 199.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road383.png: 640x480 1 speedlimit, 222.0ms\n",
            "Speed: 2.4ms preprocess, 222.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road386.png: 640x480 1 speedlimit, 200.3ms\n",
            "Speed: 2.9ms preprocess, 200.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road39.png: 640x512 1 trafficlight, 204.1ms\n",
            "Speed: 2.2ms preprocess, 204.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road392.png: 640x480 1 speedlimit, 265.4ms\n",
            "Speed: 5.2ms preprocess, 265.4ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road393.png: 640x480 1 speedlimit, 264.1ms\n",
            "Speed: 2.8ms preprocess, 264.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road394.png: 640x480 1 speedlimit, 221.9ms\n",
            "Speed: 2.1ms preprocess, 221.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road406.png: 640x480 1 speedlimit, 227.7ms\n",
            "Speed: 2.1ms preprocess, 227.7ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road41.png: 640x448 2 trafficlights, 199.6ms\n",
            "Speed: 2.1ms preprocess, 199.6ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road413.png: 640x480 1 speedlimit, 228.7ms\n",
            "Speed: 3.0ms preprocess, 228.7ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road415.png: 640x480 1 speedlimit, 214.3ms\n",
            "Speed: 2.3ms preprocess, 214.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road418.png: 640x480 1 speedlimit, 242.9ms\n",
            "Speed: 2.1ms preprocess, 242.9ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road422.png: 640x480 1 speedlimit, 245.5ms\n",
            "Speed: 2.7ms preprocess, 245.5ms inference, 3.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road423.png: 640x480 1 speedlimit, 247.6ms\n",
            "Speed: 3.5ms preprocess, 247.6ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road427.png: 640x480 1 speedlimit, 240.0ms\n",
            "Speed: 3.6ms preprocess, 240.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road43.png: 448x640 5 trafficlights, 212.6ms\n",
            "Speed: 7.6ms preprocess, 212.6ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road440.png: 640x480 1 speedlimit, 248.2ms\n",
            "Speed: 2.1ms preprocess, 248.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road441.png: 640x480 1 speedlimit, 248.2ms\n",
            "Speed: 2.4ms preprocess, 248.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road455.png: 640x480 1 speedlimit, 227.0ms\n",
            "Speed: 2.2ms preprocess, 227.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road457.png: 640x480 1 speedlimit, 257.2ms\n",
            "Speed: 2.3ms preprocess, 257.2ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road463.png: 640x480 1 trafficlight, 1 speedlimit, 1 crosswalk, 273.0ms\n",
            "Speed: 3.2ms preprocess, 273.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road466.png: 640x480 1 trafficlight, 1 speedlimit, 1 crosswalk, 228.6ms\n",
            "Speed: 3.7ms preprocess, 228.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road467.png: 640x480 1 speedlimit, 244.9ms\n",
            "Speed: 2.9ms preprocess, 244.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road473.png: 640x480 1 speedlimit, 253.6ms\n",
            "Speed: 2.1ms preprocess, 253.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road479.png: 640x480 1 speedlimit, 222.4ms\n",
            "Speed: 2.2ms preprocess, 222.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road481.png: 640x480 1 speedlimit, 1 crosswalk, 238.6ms\n",
            "Speed: 2.3ms preprocess, 238.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road484.png: 640x480 1 speedlimit, 228.4ms\n",
            "Speed: 2.3ms preprocess, 228.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road486.png: 640x480 1 speedlimit, 285.3ms\n",
            "Speed: 2.9ms preprocess, 285.3ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road487.png: 640x480 1 speedlimit, 317.4ms\n",
            "Speed: 3.3ms preprocess, 317.4ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road495.png: 640x480 1 trafficlight, 1 crosswalk, 313.0ms\n",
            "Speed: 5.1ms preprocess, 313.0ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road5.png: 640x448 1 trafficlight, 243.6ms\n",
            "Speed: 3.3ms preprocess, 243.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road500.png: 640x480 1 speedlimit, 263.9ms\n",
            "Speed: 3.0ms preprocess, 263.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road501.png: 640x480 1 speedlimit, 269.0ms\n",
            "Speed: 2.2ms preprocess, 269.0ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road51.png: 640x640 1 trafficlight, 305.8ms\n",
            "Speed: 5.8ms preprocess, 305.8ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road524.png: 640x480 1 speedlimit, 268.0ms\n",
            "Speed: 3.9ms preprocess, 268.0ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road536.png: 640x480 1 speedlimit, 1 crosswalk, 263.9ms\n",
            "Speed: 2.5ms preprocess, 263.9ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road54.png: 640x480 1 stop, 257.9ms\n",
            "Speed: 2.5ms preprocess, 257.9ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road549.png: 640x480 1 speedlimit, 1 crosswalk, 214.1ms\n",
            "Speed: 3.6ms preprocess, 214.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road55.png: 448x640 1 stop, 291.2ms\n",
            "Speed: 2.5ms preprocess, 291.2ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road555.png: 640x480 1 speedlimit, 188.6ms\n",
            "Speed: 2.1ms preprocess, 188.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road566.png: 640x480 1 speedlimit, 209.9ms\n",
            "Speed: 2.8ms preprocess, 209.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road569.png: 640x480 1 speedlimit, 202.8ms\n",
            "Speed: 2.5ms preprocess, 202.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road57.png: 640x480 1 stop, 197.4ms\n",
            "Speed: 4.3ms preprocess, 197.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road574.png: 640x480 1 speedlimit, 1 crosswalk, 211.6ms\n",
            "Speed: 2.2ms preprocess, 211.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road575.png: 640x480 1 speedlimit, 1 crosswalk, 245.8ms\n",
            "Speed: 2.4ms preprocess, 245.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road580.png: 640x480 2 crosswalks, 202.4ms\n",
            "Speed: 2.1ms preprocess, 202.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road584.png: 640x480 1 speedlimit, 202.9ms\n",
            "Speed: 2.4ms preprocess, 202.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road595.png: 640x480 1 speedlimit, 245.8ms\n",
            "Speed: 3.6ms preprocess, 245.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road60.png: 640x640 2 stops, 311.9ms\n",
            "Speed: 8.6ms preprocess, 311.9ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road61.png: 512x640 1 stop, 285.5ms\n",
            "Speed: 2.7ms preprocess, 285.5ms inference, 1.6ms postprocess per image at shape (1, 3, 512, 640)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road610.png: 640x480 1 speedlimit, 244.7ms\n",
            "Speed: 3.2ms preprocess, 244.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road616.png: 640x480 2 speedlimits, 1 crosswalk, 225.3ms\n",
            "Speed: 2.2ms preprocess, 225.3ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road617.png: 640x480 1 speedlimit, 1 crosswalk, 254.5ms\n",
            "Speed: 2.7ms preprocess, 254.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road629.png: 640x480 1 speedlimit, 278.2ms\n",
            "Speed: 4.5ms preprocess, 278.2ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road632.png: 640x480 1 stop, 260.3ms\n",
            "Speed: 2.6ms preprocess, 260.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road64.png: 512x640 1 stop, 255.0ms\n",
            "Speed: 2.3ms preprocess, 255.0ms inference, 1.6ms postprocess per image at shape (1, 3, 512, 640)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road652.png: 640x480 2 speedlimits, 1 crosswalk, 202.9ms\n",
            "Speed: 2.9ms preprocess, 202.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road653.png: 640x480 2 speedlimits, 1 crosswalk, 227.8ms\n",
            "Speed: 2.7ms preprocess, 227.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road658.png: 640x480 2 speedlimits, 1 crosswalk, 240.1ms\n",
            "Speed: 2.2ms preprocess, 240.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road661.png: 640x480 1 speedlimit, 204.3ms\n",
            "Speed: 2.2ms preprocess, 204.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road665.png: 640x480 1 speedlimit, 276.2ms\n",
            "Speed: 3.2ms preprocess, 276.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road669.png: 640x480 1 speedlimit, 1 crosswalk, 186.0ms\n",
            "Speed: 2.1ms preprocess, 186.0ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road673.png: 640x480 1 stop, 1 speedlimit, 197.7ms\n",
            "Speed: 4.1ms preprocess, 197.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road675.png: 640x480 1 stop, 1 speedlimit, 155.3ms\n",
            "Speed: 2.4ms preprocess, 155.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road678.png: 640x480 1 speedlimit, 165.8ms\n",
            "Speed: 2.2ms preprocess, 165.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road683.png: 640x480 1 speedlimit, 167.7ms\n",
            "Speed: 2.1ms preprocess, 167.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road684.png: 640x480 1 speedlimit, 176.4ms\n",
            "Speed: 2.1ms preprocess, 176.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road687.png: 640x480 2 speedlimits, 188.9ms\n",
            "Speed: 2.2ms preprocess, 188.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road692.png: 640x480 2 speedlimits, 182.8ms\n",
            "Speed: 2.2ms preprocess, 182.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road7.png: 640x512 2 trafficlights, 205.4ms\n",
            "Speed: 2.5ms preprocess, 205.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road703.png: 640x480 2 speedlimits, 207.2ms\n",
            "Speed: 2.3ms preprocess, 207.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road706.png: 640x480 2 speedlimits, 204.0ms\n",
            "Speed: 2.3ms preprocess, 204.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road708.png: 640x480 2 speedlimits, 257.3ms\n",
            "Speed: 2.3ms preprocess, 257.3ms inference, 3.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road71.png: 512x640 1 stop, 200.7ms\n",
            "Speed: 2.3ms preprocess, 200.7ms inference, 1.8ms postprocess per image at shape (1, 3, 512, 640)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road712.png: 640x480 2 speedlimits, 225.1ms\n",
            "Speed: 2.3ms preprocess, 225.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road735.png: 640x480 3 speedlimits, 256.0ms\n",
            "Speed: 3.7ms preprocess, 256.0ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road738.png: 640x480 2 speedlimits, 200.0ms\n",
            "Speed: 2.3ms preprocess, 200.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road742.png: 640x480 2 speedlimits, 199.8ms\n",
            "Speed: 2.1ms preprocess, 199.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road743.png: 640x480 2 speedlimits, 188.4ms\n",
            "Speed: 2.3ms preprocess, 188.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road749.png: 640x480 2 speedlimits, 215.1ms\n",
            "Speed: 3.8ms preprocess, 215.1ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road754.png: 640x480 2 speedlimits, 200.5ms\n",
            "Speed: 2.3ms preprocess, 200.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road757.png: 640x480 1 speedlimit, 185.4ms\n",
            "Speed: 3.2ms preprocess, 185.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road76.png: 448x640 1 stop, 181.9ms\n",
            "Speed: 2.0ms preprocess, 181.9ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road760.png: 640x480 1 speedlimit, 199.3ms\n",
            "Speed: 5.6ms preprocess, 199.3ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road763.png: 640x480 1 speedlimit, 192.5ms\n",
            "Speed: 2.6ms preprocess, 192.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road766.png: 640x480 1 speedlimit, 200.0ms\n",
            "Speed: 2.1ms preprocess, 200.0ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road779.png: 640x480 1 speedlimit, 192.7ms\n",
            "Speed: 3.0ms preprocess, 192.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road783.png: 640x480 1 speedlimit, 216.3ms\n",
            "Speed: 2.1ms preprocess, 216.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road795.png: 640x480 1 speedlimit, 202.3ms\n",
            "Speed: 2.3ms preprocess, 202.3ms inference, 2.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road801.png: 640x480 1 speedlimit, 310.6ms\n",
            "Speed: 2.4ms preprocess, 310.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road808.png: 640x480 1 speedlimit, 212.6ms\n",
            "Speed: 2.5ms preprocess, 212.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road81.png: 640x480 1 stop, 198.7ms\n",
            "Speed: 3.0ms preprocess, 198.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road820.png: 640x480 7 trafficlights, 1 stop, 2 crosswalks, 181.4ms\n",
            "Speed: 2.4ms preprocess, 181.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road825.png: 640x480 3 trafficlights, 1 stop, 2 crosswalks, 204.7ms\n",
            "Speed: 2.2ms preprocess, 204.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road826.png: 640x480 2 trafficlights, 1 stop, 2 crosswalks, 195.7ms\n",
            "Speed: 3.9ms preprocess, 195.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 c:\\Users\\afraz\\Desktop\\Nexa_eval\\Task_05\\YOLO_Format_Dataset\\val\\images\\road827.png: 640x480 1 speedlimit, 225.2ms\n",
            "Speed: 2.4ms preprocess, 225.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Run prediction and show/save results\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m img_path \u001b[38;5;129;01min\u001b[39;00m image_paths:\n\u001b[32m     32\u001b[39m     \u001b[38;5;66;03m# Run inference\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     results = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m     \u001b[38;5;66;03m# Get annotated frame with bounding boxes\u001b[39;00m\n\u001b[32m     36\u001b[39m     annotated_image = results[\u001b[32m0\u001b[39m].plot()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\afraz\\anaconda3\\envs\\main_env\\Lib\\site-packages\\ultralytics\\engine\\model.py:185\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, source, stream, **kwargs)\u001b[39m\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m    157\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    158\u001b[39m     source: Union[\u001b[38;5;28mstr\u001b[39m, Path, \u001b[38;5;28mint\u001b[39m, Image.Image, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np.ndarray, torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    159\u001b[39m     stream: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    160\u001b[39m     **kwargs: Any,\n\u001b[32m    161\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m:\n\u001b[32m    162\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[33;03m    Alias for the predict method, enabling the model instance to be callable for predictions.\u001b[39;00m\n\u001b[32m    164\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    183\u001b[39m \u001b[33;03m        ...     print(f\"Detected {len(r)} objects in image\")\u001b[39;00m\n\u001b[32m    184\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\afraz\\anaconda3\\envs\\main_env\\Lib\\site-packages\\ultralytics\\engine\\model.py:555\u001b[39m, in \u001b[36mModel.predict\u001b[39m\u001b[34m(self, source, stream, predictor, **kwargs)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.predictor, \u001b[33m\"\u001b[39m\u001b[33mset_prompts\u001b[39m\u001b[33m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[32m    554\u001b[39m     \u001b[38;5;28mself\u001b[39m.predictor.set_prompts(prompts)\n\u001b[32m--> \u001b[39m\u001b[32m555\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.predictor.predict_cli(source=source) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m=\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\afraz\\anaconda3\\envs\\main_env\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:227\u001b[39m, in \u001b[36mBasePredictor.__call__\u001b[39m\u001b[34m(self, source, model, stream, *args, **kwargs)\u001b[39m\n\u001b[32m    225\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stream_inference(source, model, *args, **kwargs)\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m.stream_inference(source, model, *args, **kwargs))\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\afraz\\anaconda3\\envs\\main_env\\Lib\\site-packages\\torch\\utils\\_contextlib.py:36\u001b[39m, in \u001b[36m_wrap_generator.<locals>.generator_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     34\u001b[39m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m         response = gen.send(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m     39\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     40\u001b[39m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\afraz\\anaconda3\\envs\\main_env\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:330\u001b[39m, in \u001b[36mBasePredictor.stream_inference\u001b[39m\u001b[34m(self, source, model, *args, **kwargs)\u001b[39m\n\u001b[32m    328\u001b[39m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[32m1\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m     preds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.embed:\n\u001b[32m    332\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch.Tensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\afraz\\anaconda3\\envs\\main_env\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:182\u001b[39m, in \u001b[36mBasePredictor.inference\u001b[39m\u001b[34m(self, im, *args, **kwargs)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Run inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[32m    177\u001b[39m visualize = (\n\u001b[32m    178\u001b[39m     increment_path(\u001b[38;5;28mself\u001b[39m.save_dir / Path(\u001b[38;5;28mself\u001b[39m.batch[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]).stem, mkdir=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    179\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.visualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.source_type.tensor)\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    181\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\afraz\\anaconda3\\envs\\main_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\afraz\\anaconda3\\envs\\main_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\afraz\\anaconda3\\envs\\main_env\\Lib\\site-packages\\ultralytics\\nn\\autobackend.py:636\u001b[39m, in \u001b[36mAutoBackend.forward\u001b[39m\u001b[34m(self, im, augment, visualize, embed, **kwargs)\u001b[39m\n\u001b[32m    634\u001b[39m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[32m    635\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.nn_module:\n\u001b[32m--> \u001b[39m\u001b[32m636\u001b[39m     y = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[43m=\u001b[49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m=\u001b[49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    638\u001b[39m \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[32m    639\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.jit:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\afraz\\anaconda3\\envs\\main_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\afraz\\anaconda3\\envs\\main_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\afraz\\anaconda3\\envs\\main_env\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:138\u001b[39m, in \u001b[36mBaseModel.forward\u001b[39m\u001b[34m(self, x, *args, **kwargs)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[32m    137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loss(x, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\afraz\\anaconda3\\envs\\main_env\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:156\u001b[39m, in \u001b[36mBaseModel.predict\u001b[39m\u001b[34m(self, x, profile, visualize, augment, embed)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[32m    155\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._predict_augment(x)\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\afraz\\anaconda3\\envs\\main_env\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:179\u001b[39m, in \u001b[36mBaseModel._predict_once\u001b[39m\u001b[34m(self, x, profile, visualize, embed)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28mself\u001b[39m._profile_one_layer(m, x, dt)\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m x = \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[32m    180\u001b[39m y.append(x \u001b[38;5;28;01mif\u001b[39;00m m.i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.save \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\afraz\\anaconda3\\envs\\main_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\afraz\\anaconda3\\envs\\main_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\afraz\\anaconda3\\envs\\main_env\\Lib\\site-packages\\ultralytics\\nn\\modules\\block.py:319\u001b[39m, in \u001b[36mC2f.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    317\u001b[39m y = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m.cv1(x).chunk(\u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m))\n\u001b[32m    318\u001b[39m y.extend(m(y[-\u001b[32m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.m)\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\afraz\\anaconda3\\envs\\main_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\afraz\\anaconda3\\envs\\main_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\afraz\\anaconda3\\envs\\main_env\\Lib\\site-packages\\ultralytics\\nn\\modules\\conv.py:92\u001b[39m, in \u001b[36mConv.forward_fuse\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward_fuse\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     83\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     84\u001b[39m \u001b[33;03m    Apply convolution and activation without batch normalization.\u001b[39;00m\n\u001b[32m     85\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     90\u001b[39m \u001b[33;03m        (torch.Tensor): Output tensor.\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.act(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\afraz\\anaconda3\\envs\\main_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\afraz\\anaconda3\\envs\\main_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\afraz\\anaconda3\\envs\\main_env\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\afraz\\anaconda3\\envs\\main_env\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    538\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    539\u001b[39m         F.pad(\n\u001b[32m    540\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    547\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    548\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# ------------------------\n",
        "# âœ… Configuration Section\n",
        "# ------------------------\n",
        "\n",
        "# Path to your trained model\n",
        "MODEL_PATH = \"yolov8small.pt\"  # or wherever your model is saved\n",
        "\n",
        "# Path to the folder containing images\n",
        "IMAGE_FOLDER = r\"YOLO_Format_Dataset\\val\\images\"  # replace with your folder name\n",
        "\n",
        "\n",
        "SAVE_FOLDER = \"Small_model_results\"\n",
        "os.makedirs(SAVE_FOLDER, exist_ok=True)\n",
        "\n",
        "# -------------------------\n",
        "# âœ… Load Model and Predict\n",
        "# -------------------------\n",
        "\n",
        "# Load the trained model\n",
        "model = YOLO(MODEL_PATH)\n",
        "\n",
        "# Get all image paths from folder\n",
        "image_paths = list(Path(IMAGE_FOLDER).glob(\"*.png\")) + list(Path(IMAGE_FOLDER).glob(\"*.png\"))\n",
        "\n",
        "# Run prediction and show/save results\n",
        "for img_path in image_paths:\n",
        "    # Run inference\n",
        "    results = model(img_path)\n",
        "\n",
        "    # Get annotated frame with bounding boxes\n",
        "    annotated_image = results[0].plot()\n",
        "\n",
        "    # Show result in a window (500ms)\n",
        "    cv2.imshow(\"Prediction\", annotated_image)\n",
        "    cv2.waitKey(500)  # show for 500 milliseconds\n",
        "\n",
        "    # Save result to disk\n",
        "    output_path = Path(SAVE_FOLDER) / img_path.name\n",
        "    cv2.imwrite(str(output_path), annotated_image)\n",
        "\n",
        "cv2.destroyAllWindows()\n",
        "print(f\"\\nâœ… All predictions saved to: {SAVE_FOLDER}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "main_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
